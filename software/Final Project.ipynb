{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 設定中文字型為 SimSun\n",
    "import matplotlib\n",
    "# matplotlib.rcParams['font.sans-serif'] = ['SimSun']\n",
    "# matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 本地資料夾位置\n",
    "train_dir = \"./all_data/train_resized\"\n",
    "test_dir = \"./all_data/test_resized\"\n",
    "\n",
    "# 定義增強生成器和原始生成器\n",
    "train_datagen = ImageDataGenerator(\n",
    "    #rescale=1.0/255,\n",
    "    rotation_range=20,  # 增強旋轉角度\n",
    "    width_shift_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[1.0, 1.75],  # 調整亮度範圍\n",
    "    #channel_shift_range=55.0,  # 調整色調\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "# 創建原始數據生成器（只進行像素縮放）\n",
    "original_datagen = ImageDataGenerator(validation_split=0.2)\n",
    "\n",
    "print(\"增強生成器設定完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增強後訓練生成器\n",
    "augmented_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(100, 100),\n",
    "    color_mode='grayscale',  # 灰階模式\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# 原始圖片的生成器\n",
    "original_generator = original_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(100, 100),\n",
    "    color_mode='grayscale',  # 灰階模式\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# 將原始圖片和增強圖片結合\n",
    "def combined_generator(original_gen, augmented_gen):\n",
    "    while True:\n",
    "        original_batch = original_gen.__next__()\n",
    "        augmented_batch = augmented_gen.__next__()\n",
    "        # 合併原始和增強圖片\n",
    "        images = np.concatenate((original_batch[0], augmented_batch[0]), axis=0)\n",
    "        labels = np.concatenate((original_batch[1], augmented_batch[1]), axis=0)\n",
    "        yield images, labels\n",
    "\n",
    "# 使用合併後的生成器進行訓練\n",
    "train_dataset = combined_generator(original_generator, augmented_generator)\n",
    "\n",
    "# 驗證圖縮放\n",
    "validation_dataset = original_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(100, 100),\n",
    "    color_mode='grayscale',  # 灰階模式\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(100, 100),\n",
    "    color_mode='grayscale',  # 灰階模式\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # 確保標籤為 one-hot 格式\n",
    "    shuffle=False  # 測試資料不需要打亂\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Dropout\n",
    "\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "batch_size = 16\n",
    "steps_per_epoch = len(original_generator)\n",
    "validation_steps = None\n",
    "\n",
    "# 定義模型\n",
    "def build_model(input_shape):\n",
    "    class QuantizeLayer(layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            # Scale to [0, 127] range\n",
    "            min_val = tf.reduce_min(inputs)\n",
    "            max_val = tf.reduce_max(inputs)\n",
    "            scaled = (inputs - min_val) / (max_val - min_val) * 127.0\n",
    "            return tf.clip_by_value(tf.round(scaled), 0.0, 127.0)\n",
    "    \n",
    "    model = Sequential([\n",
    "        # 卷積層 1 + 激活函數 + 量化\n",
    "        layers.Conv2D(4, (5, 5), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # QuantizeLayer(),\n",
    "\n",
    "        # 卷積層 2 + 激活函數 + 量化\n",
    "        layers.Conv2D(8, (5, 5), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # QuantizeLayer(),\n",
    "\n",
    "        # 卷積層 3 + 激活函數 + 量化\n",
    "        layers.Conv2D(8, (5, 5), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # QuantizeLayer(),\n",
    "\n",
    "        # 卷積層 4 + 激活函數 + 量化\n",
    "        layers.Conv2D(8, (5, 5), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # QuantizeLayer(),\n",
    "\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        Dense(16, activation='relu'),\n",
    "        # QuantizeLayer(),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# 指定輸入形狀\n",
    "input_shape = (100, 100, 1)\n",
    "\n",
    "# 建立模型\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# 編譯模型\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 配置回調函數\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=4,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# 模型訓練\n",
    "history = model.fit(\n",
    "    train_dataset,  # 訓練數據\n",
    "    steps_per_epoch=steps_per_epoch,  # 每個 epoch 的步數\n",
    "    epochs=20,\n",
    "    # epochs=5,\n",
    "    validation_data=validation_dataset,  # 驗證數據\n",
    "    #callbacks=[early_stopping, reduce_lr],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# 載入模型\n",
    "model = load_model('model.h5')\n",
    "# 定義輸出資料夾\n",
    "output_dir = \"model_parameters\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # 確保輸出資料夾存在\n",
    "\n",
    "# 導出模型參數\n",
    "for idx, layer in enumerate(model.layers):\n",
    "    # 確認該層是否有權重參數\n",
    "    weights = layer.get_weights()\n",
    "    if weights:  # 如果該層有參數（如卷積層、Dense層等）\n",
    "        weights_file = os.path.join(output_dir, f\"layer_{idx}_weights.dat\")\n",
    "        biases_file = os.path.join(output_dir, f\"layer_{idx}_biases.dat\")\n",
    "        \n",
    "        # 保存權重\n",
    "        np.savetxt(weights_file, weights[0].flatten(), delimiter=',', fmt='%.6f')\n",
    "        print(f\"Saved weights for layer {idx} to {weights_file}\")\n",
    "        \n",
    "        # 保存偏置\n",
    "        if len(weights) > 1:  # 如果有偏置參數\n",
    "            np.savetxt(biases_file, weights[1], delimiter=',', fmt='%.6f')\n",
    "            print(f\"Saved biases for layer {idx} to {biases_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 設置字體為 DejaVu Sans\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# 繪製損失曲線\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curve')\n",
    "plt.savefig(\"loss_curve2.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Curve')\n",
    "plt.savefig(\"accuracy_curve2.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./all_data/train_resized\"\n",
    "test_dir = \"./all_data/test_resized\"\n",
    "\n",
    "class_names = ['A2C', 'A4C', 'OTHER', 'PLAX', 'PSAX']\n",
    "# class_names = ['A2C', 'A4C', 'Other', 'PLAX', 'PSAX']\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(100, 100),\n",
    "    color_mode='grayscale',  # 灰階模式\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # 確保標籤為 one-hot 格式\n",
    "    classes=class_names,\n",
    "    # classes=['A2C', 'A4C', 'Other', 'PLAX', 'PSAX'],\n",
    "    shuffle=False  # 測試資料不需要打亂\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "# 載入模型\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# 獲取模型的預測結果\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# 取得每張圖片的預測類別\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 真實標籤\n",
    "true_classes = test_dataset.classes\n",
    "\n",
    "# 標籤名稱\n",
    "class_labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    \"File Name\": test_dataset.filenames,\n",
    "    \"True Label\": [class_labels[i] for i in true_classes],\n",
    "    \"Predicted Label\": [class_labels[i] for i in predicted_classes]\n",
    "})\n",
    "\n",
    "# Calculate the number of correct predictions\n",
    "correct_predictions = (results_df['True Label'] == results_df['Predicted Label']).sum()\n",
    "\n",
    "# Calculate the total number of predictions\n",
    "total_predictions = len(results_df)\n",
    "\n",
    "# Calculate the proportion (accuracy)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "top_2_predictions = np.argsort(predictions, axis=1)[:, -2:]\n",
    "\n",
    "# Check if the true label is among the top-5 predictions\n",
    "top_2_correct = [true_classes[i] in top_2_predictions[i] for i in range(len(true_classes))]\n",
    "\n",
    "# Calculate Top-5 Accuracy\n",
    "top_2_accuracy = np.mean(top_2_correct)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of correct predictions: {correct_predictions}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Top-2 Accuracy: {top_2_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate accuracy per class\n",
    "class_accuracies = results_df.groupby('True Label').apply(\n",
    "    lambda group: (group['True Label'] == group['Predicted Label']).mean()\n",
    ")\n",
    "\n",
    "# Convert the result to a DataFrame for plotting\n",
    "class_accuracies_df = class_accuracies.reset_index()\n",
    "class_accuracies_df.columns = ['Class', 'Accuracy']\n",
    "\n",
    "# Plot the accuracy per class\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(class_accuracies_df['Class'], class_accuracies_df['Accuracy'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Per-Class Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"per_class_accuracy.png\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(results_df['True Label'], results_df['Predicted Label'], labels=results_df['True Label'].unique())\n",
    "\n",
    "# Plot confusion matrix as a heatmap without annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, cmap=\"Blues\", cbar=True, xticklabels=results_df['True Label'].unique(), yticklabels=results_df['True Label'].unique(), annot=False)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "avsd_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
